{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a00e4d-844c-4e87-a394-e2f355d360ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Spark Session...\n",
      "======================================================================\n",
      "‚úÖ Spark Session created successfully!\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "SPARK CLUSTER INFORMATION\n",
      "======================================================================\n",
      "Spark Version: 3.5.0\n",
      "Master URL: spark://spark-master:7077\n",
      "Application Name: DataLake - Medallion Architecture\n",
      "Application ID: app-20251126184040-0000\n",
      "Default Parallelism: 2\n",
      "Spark UI: http://localhost:4040\n",
      "Master UI: http://localhost:8080\n",
      "\n",
      "üìä Cluster Status:\n",
      "‚úÖ Connected to Standalone Cluster\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TEST 1: DISTRIBUTED PROCESSING\n",
      "======================================================================\n",
      "Created RDD with 1000 elements\n",
      "Number of partitions: 6\n",
      "First partition sample: [1, 2, 3, 4, 5]\n",
      "Sum of doubled values: 1001000\n",
      "‚úÖ Distributed processing working!\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TEST 2: MINIO CONNECTION - WRITE DATA\n",
      "======================================================================\n",
      "Sample DataFrame:\n",
      "+---+-----+---+-----------+--------+\n",
      "| id| name|age| department|  salary|\n",
      "+---+-----+---+-----------+--------+\n",
      "|  1|Alice| 34|Engineering| 95000.5|\n",
      "|  2|  Bob| 45|      Sales| 78000.0|\n",
      "|  3|Cathy| 29|Engineering|88000.75|\n",
      "|  4|David| 52| Management|120000.0|\n",
      "|  5|  Eve| 38|      Sales|82000.25|\n",
      "+---+-----+---+-----------+--------+\n",
      "\n",
      "\n",
      "üìù Writing to MinIO: s3a://bronze/test_employees/\n",
      "‚úÖ Successfully wrote data to MinIO Bronze layer!\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TEST 3: MINIO CONNECTION - READ DATA\n",
      "======================================================================\n",
      "üìñ Reading from MinIO: s3a://bronze/test_employees/\n",
      "‚úÖ Successfully read 5 records from MinIO!\n",
      "\n",
      "Data from MinIO:\n",
      "+---+-----+---+-----------+--------+\n",
      "| id| name|age| department|  salary|\n",
      "+---+-----+---+-----------+--------+\n",
      "|  1|Alice| 34|Engineering| 95000.5|\n",
      "|  2|  Bob| 45|      Sales| 78000.0|\n",
      "|  3|Cathy| 29|Engineering|88000.75|\n",
      "|  4|David| 52| Management|120000.0|\n",
      "|  5|  Eve| 38|      Sales|82000.25|\n",
      "+---+-----+---+-----------+--------+\n",
      "\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TEST 4: POSTGRESQL CONNECTION - WRITE DATA\n",
      "======================================================================\n",
      "üìù Writing to PostgreSQL table: employees\n",
      "‚úÖ Successfully wrote data to PostgreSQL!\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TEST 5: POSTGRESQL CONNECTION - READ DATA\n",
      "======================================================================\n",
      "üìñ Reading from PostgreSQL table: employees\n",
      "‚úÖ Successfully read 5 records from PostgreSQL!\n",
      "\n",
      "Data from PostgreSQL:\n",
      "+---+-----+---+-----------+--------+--------------------------+\n",
      "|id |name |age|department |salary  |created_at                |\n",
      "+---+-----+---+-----------+--------+--------------------------+\n",
      "|2  |Bob  |45 |Sales      |78000.0 |2025-11-26 18:41:20.292011|\n",
      "|3  |Cathy|29 |Engineering|88000.75|2025-11-26 18:41:20.292011|\n",
      "|1  |Alice|34 |Engineering|95000.5 |2025-11-26 18:41:20.292011|\n",
      "|4  |David|52 |Management |120000.0|2025-11-26 18:41:20.292011|\n",
      "|5  |Eve  |38 |Sales      |82000.25|2025-11-26 18:41:20.292011|\n",
      "+---+-----+---+-----------+--------+--------------------------+\n",
      "\n",
      "\n",
      "Schema:\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      "\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TEST 6: COMPLETE MEDALLION PIPELINE\n",
      "======================================================================\n",
      "\n",
      "ü•â BRONZE LAYER - Raw Data\n",
      "Records: 5\n",
      "+---+-----+---+-----------+--------+\n",
      "| id| name|age| department|  salary|\n",
      "+---+-----+---+-----------+--------+\n",
      "|  1|Alice| 34|Engineering| 95000.5|\n",
      "|  2|  Bob| 45|      Sales| 78000.0|\n",
      "|  3|Cathy| 29|Engineering|88000.75|\n",
      "|  4|David| 52| Management|120000.0|\n",
      "|  5|  Eve| 38|      Sales|82000.25|\n",
      "+---+-----+---+-----------+--------+\n",
      "\n",
      "\n",
      "ü•à SILVER LAYER - Cleaned & Transformed Data\n",
      "‚úÖ Wrote to Silver layer: s3a://silver/test_employees/\n",
      "Records: 4\n",
      "+---+-----+---+-----------+--------+------------+--------------------+\n",
      "| id| name|age| department|  salary|salary_grade| processed_timestamp|\n",
      "+---+-----+---+-----------+--------+------------+--------------------+\n",
      "|  1|Alice| 34|Engineering| 95000.5|      Medium|2025-11-26 18:41:...|\n",
      "|  2|  Bob| 45|      Sales| 78000.0|         Low|2025-11-26 18:41:...|\n",
      "|  4|David| 52| Management|120000.0|        High|2025-11-26 18:41:...|\n",
      "|  5|  Eve| 38|      Sales|82000.25|      Medium|2025-11-26 18:41:...|\n",
      "+---+-----+---+-----------+--------+------------+--------------------+\n",
      "\n",
      "\n",
      "ü•á GOLD LAYER - Business Aggregations\n",
      "‚úÖ Wrote to Gold layer: s3a://gold/test_employees_summary/\n",
      "Records: 4\n",
      "+-----------+------------+--------------+----------+-------+-------+\n",
      "| department|salary_grade|employee_count|avg_salary|min_age|max_age|\n",
      "+-----------+------------+--------------+----------+-------+-------+\n",
      "|Engineering|      Medium|             1|   95000.5|     34|     34|\n",
      "| Management|        High|             1|  120000.0|     52|     52|\n",
      "|      Sales|         Low|             1|   78000.0|     45|     45|\n",
      "|      Sales|      Medium|             1|  82000.25|     38|     38|\n",
      "+-----------+------------+--------------+----------+-------+-------+\n",
      "\n",
      "‚úÖ Wrote Gold layer to PostgreSQL table: employees_summary\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "üéâ CONNECTION TEST SUMMARY\n",
      "======================================================================\n",
      "\n",
      "‚úÖ Spark Standalone Cluster - Connected\n",
      "‚úÖ MinIO (S3) - Read/Write Working\n",
      "‚úÖ PostgreSQL - Read/Write Working\n",
      "‚úÖ Medallion Pipeline - Functional\n",
      "\n",
      "üìä Data Locations:\n",
      "   Bronze: s3a://bronze/test_employees/\n",
      "   Silver: s3a://silver/test_employees/\n",
      "   Gold:   s3a://gold/test_employees_summary/\n",
      "   \n",
      "üíæ PostgreSQL Tables:\n",
      "   - employees\n",
      "   - employees_summary (for Metabase)\n",
      "\n",
      "üåê Access Points:\n",
      "   - Spark UI: http://localhost:4040\n",
      "   - Master UI: http://localhost:8080\n",
      "   - MinIO Console: http://localhost:9001\n",
      "   - JupyterLab: http://localhost:8888\n",
      "   - Metabase: http://localhost:3000\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SPARK SESSION CONFIGURATION - Connect to Cluster, MinIO, and PostgreSQL\n",
    "# ============================================================================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "print(\"Creating Spark Session...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create Spark Session with all configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataLake - Medallion Architecture\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.cores.max\", \"3\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262,\"\n",
    "            \"org.postgresql:postgresql:42.5.4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to reduce noise\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"‚úÖ Spark Session created successfully!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFY CLUSTER CONNECTION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SPARK CLUSTER INFORMATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Master URL: {sc.master}\")\n",
    "print(f\"Application Name: {sc.appName}\")\n",
    "print(f\"Application ID: {sc.applicationId}\")\n",
    "print(f\"Default Parallelism: {sc.defaultParallelism}\")\n",
    "print(f\"Spark UI: http://localhost:4040\")\n",
    "print(f\"Master UI: http://localhost:8080\")\n",
    "\n",
    "print(\"\\nüìä Cluster Status:\")\n",
    "if sc.master.startswith(\"spark://\"):\n",
    "    print(\"‚úÖ Connected to Standalone Cluster\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Running in Local Mode\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 1: VERIFY DISTRIBUTED PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST 1: DISTRIBUTED PROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create test data distributed across workers\n",
    "data = list(range(1, 1001))\n",
    "rdd = sc.parallelize(data, 6)  # 6 partitions for 3 workers\n",
    "\n",
    "print(f\"Created RDD with {rdd.count()} elements\")\n",
    "print(f\"Number of partitions: {rdd.getNumPartitions()}\")\n",
    "print(f\"First partition sample: {rdd.glom().collect()[0][:5]}\")\n",
    "\n",
    "# Simple computation\n",
    "result = rdd.map(lambda x: x * 2).sum()\n",
    "print(f\"Sum of doubled values: {result}\")\n",
    "print(\"‚úÖ Distributed processing working!\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 2: MINIO (S3) CONNECTION - WRITE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST 2: MINIO CONNECTION - WRITE DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create sample DataFrame\n",
    "sample_data = [\n",
    "    (1, \"Alice\", 34, \"Engineering\", 95000.50),\n",
    "    (2, \"Bob\", 45, \"Sales\", 78000.00),\n",
    "    (3, \"Cathy\", 29, \"Engineering\", 88000.75),\n",
    "    (4, \"David\", 52, \"Management\", 120000.00),\n",
    "    (5, \"Eve\", 38, \"Sales\", 82000.25)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), False),\n",
    "    StructField(\"department\", StringType(), False),\n",
    "    StructField(\"salary\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(sample_data, schema)\n",
    "\n",
    "print(\"Sample DataFrame:\")\n",
    "df.show()\n",
    "\n",
    "# Write to MinIO Bronze layer\n",
    "bronze_path = \"s3a://bronze/test_employees/\"\n",
    "print(f\"\\nüìù Writing to MinIO: {bronze_path}\")\n",
    "\n",
    "try:\n",
    "    df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(bronze_path)\n",
    "    print(\"‚úÖ Successfully wrote data to MinIO Bronze layer!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error writing to MinIO: {str(e)}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 3: MINIO (S3) CONNECTION - READ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST 3: MINIO CONNECTION - READ DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"üìñ Reading from MinIO: {bronze_path}\")\n",
    "\n",
    "try:\n",
    "    df_read = spark.read.parquet(bronze_path)\n",
    "    print(f\"‚úÖ Successfully read {df_read.count()} records from MinIO!\")\n",
    "    print(\"\\nData from MinIO:\")\n",
    "    df_read.show()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading from MinIO: {str(e)}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 4: POSTGRESQL CONNECTION - WRITE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST 4: POSTGRESQL CONNECTION - WRITE DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Add timestamp column\n",
    "df_with_timestamp = df.withColumn(\"created_at\", current_timestamp())\n",
    "\n",
    "# PostgreSQL connection properties\n",
    "postgres_url = \"jdbc:postgresql://postgres:5432/gold_db\"\n",
    "postgres_properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"postgres\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "table_name = \"employees\"\n",
    "\n",
    "print(f\"üìù Writing to PostgreSQL table: {table_name}\")\n",
    "\n",
    "try:\n",
    "    df_with_timestamp.write \\\n",
    "        .jdbc(url=postgres_url, \n",
    "              table=table_name, \n",
    "              mode=\"overwrite\", \n",
    "              properties=postgres_properties)\n",
    "    print(\"‚úÖ Successfully wrote data to PostgreSQL!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error writing to PostgreSQL: {str(e)}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 5: POSTGRESQL CONNECTION - READ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST 5: POSTGRESQL CONNECTION - READ DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"üìñ Reading from PostgreSQL table: {table_name}\")\n",
    "\n",
    "try:\n",
    "    df_postgres = spark.read \\\n",
    "        .jdbc(url=postgres_url, \n",
    "              table=table_name, \n",
    "              properties=postgres_properties)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully read {df_postgres.count()} records from PostgreSQL!\")\n",
    "    print(\"\\nData from PostgreSQL:\")\n",
    "    df_postgres.show(truncate=False)\n",
    "    \n",
    "    print(\"\\nSchema:\")\n",
    "    df_postgres.printSchema()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading from PostgreSQL: {str(e)}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 6: COMPLETE PIPELINE - Bronze ‚Üí Silver ‚Üí Gold\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST 6: COMPLETE MEDALLION PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# BRONZE: Raw data (already written above)\n",
    "print(\"\\nü•â BRONZE LAYER - Raw Data\")\n",
    "bronze_df = spark.read.parquet(\"s3a://bronze/test_employees/\")\n",
    "print(f\"Records: {bronze_df.count()}\")\n",
    "bronze_df.show(5)\n",
    "\n",
    "# SILVER: Clean and transform\n",
    "print(\"\\nü•à SILVER LAYER - Cleaned & Transformed Data\")\n",
    "silver_df = bronze_df \\\n",
    "    .filter(col(\"age\") >= 30) \\\n",
    "    .withColumn(\"salary_grade\", \n",
    "                when(col(\"salary\") >= 100000, \"High\")\n",
    "                .when(col(\"salary\") >= 80000, \"Medium\")\n",
    "                .otherwise(\"Low\")) \\\n",
    "    .withColumn(\"processed_timestamp\", current_timestamp())\n",
    "\n",
    "silver_path = \"s3a://silver/test_employees/\"\n",
    "silver_df.write.mode(\"overwrite\").parquet(silver_path)\n",
    "print(f\"‚úÖ Wrote to Silver layer: {silver_path}\")\n",
    "print(f\"Records: {silver_df.count()}\")\n",
    "silver_df.show()\n",
    "\n",
    "# GOLD: Aggregated business metrics\n",
    "print(\"\\nü•á GOLD LAYER - Business Aggregations\")\n",
    "gold_df = silver_df.groupBy(\"department\", \"salary_grade\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"employee_count\"),\n",
    "        avg(\"salary\").alias(\"avg_salary\"),\n",
    "        min(\"age\").alias(\"min_age\"),\n",
    "        max(\"age\").alias(\"max_age\")\n",
    "    ) \\\n",
    "    .orderBy(\"department\", \"salary_grade\")\n",
    "\n",
    "gold_path = \"s3a://gold/test_employees_summary/\"\n",
    "gold_df.write.mode(\"overwrite\").parquet(gold_path)\n",
    "print(f\"‚úÖ Wrote to Gold layer: {gold_path}\")\n",
    "print(f\"Records: {gold_df.count()}\")\n",
    "gold_df.show()\n",
    "\n",
    "# Write Gold layer to PostgreSQL for Metabase\n",
    "gold_table = \"employees_summary\"\n",
    "gold_df.write \\\n",
    "    .jdbc(url=postgres_url, \n",
    "          table=gold_table, \n",
    "          mode=\"overwrite\", \n",
    "          properties=postgres_properties)\n",
    "print(f\"‚úÖ Wrote Gold layer to PostgreSQL table: {gold_table}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéâ CONNECTION TEST SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary = \"\"\"\n",
    "‚úÖ Spark Standalone Cluster - Connected\n",
    "‚úÖ MinIO (S3) - Read/Write Working\n",
    "‚úÖ PostgreSQL - Read/Write Working\n",
    "‚úÖ Medallion Pipeline - Functional\n",
    "\n",
    "üìä Data Locations:\n",
    "   Bronze: s3a://bronze/test_employees/\n",
    "   Silver: s3a://silver/test_employees/\n",
    "   Gold:   s3a://gold/test_employees_summary/\n",
    "   \n",
    "üíæ PostgreSQL Tables:\n",
    "   - employees\n",
    "   - employees_summary (for Metabase)\n",
    "\n",
    "üåê Access Points:\n",
    "   - Spark UI: http://localhost:4040\n",
    "   - Master UI: http://localhost:8080\n",
    "   - MinIO Console: http://localhost:9001\n",
    "   - JupyterLab: http://localhost:8888\n",
    "   - Metabase: http://localhost:3000\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e2c4b70-4d00-4705-a43d-dc37c5f60646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(f\"PySpark version: {pyspark.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79df22f6-9d5d-4b2c-808c-c4b785b3ed00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sql extension is already loaded. To reload it, use:\n",
      "  %reload_ext sql\n"
     ]
    }
   ],
   "source": [
    "%load_ext sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bd215f1-1a10-4423-8620-657b283011eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sql/connection.py\", line 45, in __init__\n",
      "    engine = sqlalchemy.create_engine(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<string>\", line 2, in create_engine\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sqlalchemy/util/deprecations.py\", line 281, in warned\n",
      "    return fn(*args, **kwargs)  # type: ignore[no-any-return]\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sqlalchemy/engine/create.py\", line 552, in create_engine\n",
      "    entrypoint = u._get_entrypoint()\n",
      "                 ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sqlalchemy/engine/url.py\", line 754, in _get_entrypoint\n",
      "    cls = registry.load(name)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py\", line 367, in load\n",
      "    raise exc.NoSuchModuleError(\n",
      "sqlalchemy.exc.NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:spark\n",
      "\n",
      "Connection info needed in SQLAlchemy format, example:\n",
      "               postgresql://username:password@hostname/dbname\n",
      "               or an existing connection: dict_keys([])\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sql/magic.py\", line 196, in execute\n",
      "    conn = sql.connection.Connection.set(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sql/connection.py\", line 70, in set\n",
      "    cls.current = existing or Connection(descriptor, connect_args, creator)\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sql/connection.py\", line 45, in __init__\n",
      "    engine = sqlalchemy.create_engine(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<string>\", line 2, in create_engine\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sqlalchemy/util/deprecations.py\", line 281, in warned\n",
      "    return fn(*args, **kwargs)  # type: ignore[no-any-return]\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sqlalchemy/engine/create.py\", line 552, in create_engine\n",
      "    entrypoint = u._get_entrypoint()\n",
      "                 ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sqlalchemy/engine/url.py\", line 754, in _get_entrypoint\n",
      "    cls = registry.load(name)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sqlalchemy/util/langhelpers.py\", line 367, in load\n",
      "    raise exc.NoSuchModuleError(\n",
      "sqlalchemy.exc.NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:spark\n",
      "\n",
      "Connection info needed in SQLAlchemy format, example:\n",
      "               postgresql://username:password@hostname/dbname\n",
      "               or an existing connection: dict_keys([])\n"
     ]
    }
   ],
   "source": [
    "%sql spark://"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "154cc6e6-823b-4990-8009-562465817e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(\"s3a://dev/transactions/\").createOrReplaceTempView(\"transactions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce165e4-f867-4686-9599-d4796ead37d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sql/magic.py\", line 196, in execute\n",
      "    conn = sql.connection.Connection.set(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/sql/connection.py\", line 82, in set\n",
      "    raise ConnectionError(\n",
      "sql.connection.ConnectionError: Environment variable $DATABASE_URL not set, and no connect string given.\n",
      "\n",
      "Connection info needed in SQLAlchemy format, example:\n",
      "               postgresql://username:password@hostname/dbname\n",
      "               or an existing connection: dict_keys([])\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "select * from transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88c1eebc-d6b0-42c3-8764-d8a24d286a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- client_id: string (nullable = true)\n",
      " |-- card_id: string (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- use_chip: string (nullable = true)\n",
      " |-- merchant_id: string (nullable = true)\n",
      " |-- merchant_city: string (nullable = true)\n",
      " |-- merchant_state: string (nullable = true)\n",
      " |-- zip: string (nullable = true)\n",
      " |-- mcc: string (nullable = true)\n",
      " |-- errors: string (nullable = true)\n",
      "\n",
      "‚úì Setup complete. Use sql('SELECT * FROM customers') to query\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "# Your existing Spark session\n",
    "# Create Spark Session with all configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataLake - Medallion Architecture\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.cores.max\", \"3\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262,\"\n",
    "            \"org.postgresql:postgresql:42.5.4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Simple SQL function\n",
    "def sql(query):\n",
    "    \"\"\"Execute Spark SQL and return as pandas DataFrame\"\"\"\n",
    "    return spark.sql(query).toPandas()\n",
    "\n",
    "# Register your parquet files as tables\n",
    "df = spark.read.csv(\"s3a://dev/transactions/\", header=True)\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "print(\"‚úì Setup complete. Use sql('SELECT * FROM customers') to query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d5fb236-94d3-4423-8e0a-d54c3d13b16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>3006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>7531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>8111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>5733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>4411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mcc\n",
       "0    8931\n",
       "1    5732\n",
       "2    6300\n",
       "3    5045\n",
       "4    5211\n",
       "..    ...\n",
       "104  3006\n",
       "105  7531\n",
       "106  8111\n",
       "107  5733\n",
       "108  4411\n",
       "\n",
       "[109 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(\"select distinct mcc from transactions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
