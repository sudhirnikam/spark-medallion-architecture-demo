{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ae930d-3130-4e1b-b755-3d281a58ff05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 08:28:24,502 - __main__ - INFO - ======================================================================\n",
      "2025-12-27 08:28:24,503 - __main__ - INFO - Starting Gold Layer ETL Job\n",
      "2025-12-27 08:28:24,504 - __main__ - INFO - ======================================================================\n",
      "2025-12-27 08:28:24,505 - __main__ - INFO - Creating Spark session\n",
      "2025-12-27 08:28:30,215 - __main__ - INFO - Spark session created: 3.5.0\n",
      "2025-12-27 08:28:30,216 - __main__ - INFO - Reading transactions from s3a://silver/transactions\n",
      "2025-12-27 08:28:47,364 - __main__ - INFO - Reading users from s3a://silver/users\n",
      "2025-12-27 08:28:50,100 - __main__ - INFO - Reading cards from s3a://silver/cards\n",
      "2025-12-27 08:28:50,419 - __main__ - INFO - Reading MCC codes from s3a://silver/mcc_codes\n",
      "2025-12-27 08:28:54,674 - __main__ - INFO - Loaded - Transactions: 13305915, Users: 2000, Cards: 6146, MCC Codes: 109\n",
      "2025-12-27 08:28:54,676 - __main__ - INFO - Starting daily aggregation with all tables\n",
      "2025-12-27 08:30:56,381 - __main__ - INFO - Enriched dataset created with 13305915 records\n",
      "2025-12-27 08:34:59,186 - __main__ - INFO - aggregation completed: 3591 daily records\n",
      "2025-12-27 08:34:59,190 - __main__ - INFO - Sample aggregated data:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+-----+------------+-----------+----------+------------------+-------------+----------------------+-----------------------+-------------------+-------------------+----------------+----------------+--------+----------+-----------------+------------------------+----------------+---------------+--------+--------------+-----------------------+----------------+----------------------+----------------+---------------+------------------+-------------------+-------------+-------------+---------+--------+----------------------+---------------------+-------------------+------------+------------+----------------+-----------------+------------------+------------+-------------+------------+----------+------------+--------------------------+\n",
      "|transaction_date|year|month|week_of_year|day_of_week|is_weekend|total_transactions|total_revenue|total_revenue_absolute|average_transaction_amt|min_transaction_amt|max_transaction_amt|avg_customer_age|avg_credit_score|pct_male|pct_female|avg_yearly_income|avg_debt_to_income_ratio|pct_credit_cards|pct_debit_cards|pct_visa|pct_mastercard|cards_on_dark_web_count|avg_credit_limit|avg_credit_utilization|top_mcc_category|top_mcc_revenue|restaurant_revenue|gas_station_revenue|unique_states|unique_cities|top_state|top_city|high_risk_transactions|swipe_over_1000_count|failed_transactions|active_users|active_cards|unique_merchants|chip_transactions|swipe_transactions|chip_revenue|swipe_revenue|total_errors|error_rate|success_rate|created_at                |\n",
      "+----------------+----+-----+------------+-----------+----------+------------------+-------------+----------------------+-----------------------+-------------------+-------------------+----------------+----------------+--------+----------+-----------------+------------------------+----------------+---------------+--------+--------------+-----------------------+----------------+----------------------+----------------+---------------+------------------+-------------------+-------------+-------------+---------+--------+----------------------+---------------------+-------------------+------------+------------+----------------+-----------------+------------------+------------+-------------+------------+----------+------------+--------------------------+\n",
      "|2010-01-01      |2010|Jan  |53          |Friday     |false     |3463              |124498.32    |178270.32             |35.95                  |-500.00            |1309.71            |55.46           |714.15          |48.48   |51.52     |46622.72         |1.21                    |30.52           |69.48          |35.72   |55.1          |0                      |15180.86        |4.28                  |Money Transfer  |15460.00       |11011.10          |6694.70            |53           |1066         |NULL     |ONLINE  |2                     |2                    |48                 |978         |1558        |1010            |0                |3015              |0.00        |104271.62    |48          |1.39      |98.61       |2025-12-27 08:30:58.733187|\n",
      "|2010-01-02      |2010|Jan  |53          |Saturday   |true      |2989              |138700.62    |162766.62             |46.40                  |-490.00            |1411.14            |55.15           |713.8           |49.72   |50.28     |46562.89         |1.22                    |32.18           |67.82          |37.64   |53.36         |0                      |15183.14        |3.23                  |Money Transfer  |13520.00       |5547.56           |6211.87            |53           |1054         |NULL     |ONLINE  |3                     |3                    |50                 |954         |1482        |986             |0                |2630              |0.00        |119035.39    |50          |1.67      |98.33       |2025-12-27 08:30:58.733187|\n",
      "|2010-01-03      |2010|Jan  |53          |Sunday     |true      |3311              |135016.77    |162940.77             |40.78                  |-492.00            |1412.64            |55.11           |715.55          |49.47   |50.53     |45910.75         |1.20                    |30.26           |69.74          |36.42   |53.55         |0                      |15235.44        |3.71                  |Money Transfer  |12460.00       |7286.80           |6429.71            |54           |1060         |CA       |ONLINE  |0                     |0                    |48                 |983         |1549        |1009            |0                |2976              |0.00        |115608.03    |48          |1.45      |98.55       |2025-12-27 08:30:58.733187|\n",
      "+----------------+----+-----+------------+-----------+----------+------------------+-------------+----------------------+-----------------------+-------------------+-------------------+----------------+----------------+--------+----------+-----------------+------------------------+----------------+---------------+--------+--------------+-----------------------+----------------+----------------------+----------------+---------------+------------------+-------------------+-------------+-------------+---------+--------+----------------------+---------------------+-------------------+------------+------------+----------------+-----------------+------------------+------------+-------------+------------+----------+------------+--------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-27 08:35:01,276 - __main__ - INFO - Writing 3591 records to PostgreSQL table: daily_transaction_summary\n",
      "2025-12-27 08:35:04,354 - __main__ - INFO - Successfully wrote data to PostgreSQL\n",
      "2025-12-27 08:35:04,355 - __main__ - INFO - ======================================================================\n",
      "2025-12-27 08:35:04,357 - __main__ - INFO - ETL Job Completed Successfully!\n",
      "2025-12-27 08:35:04,357 - __main__ - INFO - Total Duration: 399.852901 seconds\n",
      "2025-12-27 08:35:04,831 - __main__ - INFO - Daily Records Created: 3591\n",
      "2025-12-27 08:35:04,832 - __main__ - INFO - ======================================================================\n",
      "2025-12-27 08:35:05,198 - __main__ - INFO - Spark session stopped\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, sum as _sum, avg, min as _min, max as _max,\n",
    "    round as _round, abs as _abs, when, dayofweek, weekofyear,\n",
    "    year, month, dayofmonth, date_format, current_timestamp,\n",
    "    countDistinct, first, desc, row_number\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration for the Gold Layer ETL job\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # MinIO/S3 Configuration\n",
    "        self.silver_base_path = \"s3a://silver\"\n",
    "        self.transactions_path = f\"{self.silver_base_path}/transactions\"\n",
    "        self.cards_path = f\"{self.silver_base_path}/cards\"\n",
    "        self.users_path = f\"{self.silver_base_path}/users\"\n",
    "        self.mcc_codes_path = f\"{self.silver_base_path}/mcc_codes\"\n",
    "        \n",
    "        # PostgreSQL Configuration\n",
    "        self.postgres_host = \"postgres\"\n",
    "        self.postgres_port = \"5432\"\n",
    "        self.postgres_db = \"gold_db\"\n",
    "        self.postgres_user = \"postgres\"\n",
    "        self.postgres_password = \"postgres\"\n",
    "        self.postgres_table = \"daily_transaction_summary\"\n",
    "        \n",
    "        # JDBC URL\n",
    "        self.jdbc_url = f\"jdbc:postgresql://{self.postgres_host}:{self.postgres_port}/{self.postgres_db}\"\n",
    "        \n",
    "        # JDBC Properties\n",
    "        self.jdbc_properties = {\n",
    "            \"user\": self.postgres_user,\n",
    "            \"password\": self.postgres_password,\n",
    "            \"driver\": \"org.postgresql.Driver\"\n",
    "        }\n",
    "\n",
    "\n",
    "class SilverDataReader:\n",
    "    \"\"\"Reads data from Silver layer\"\"\"\n",
    "    \n",
    "    def __init__(self, spark: SparkSession, config: Config):\n",
    "        self.spark = spark\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def read_transactions(self):\n",
    "        \"\"\"Read transactions from silver layer\"\"\"\n",
    "        self.logger.info(f\"Reading transactions from {self.config.transactions_path}\")\n",
    "        return self.spark.read.parquet(self.config.transactions_path)\n",
    "    \n",
    "    def read_cards(self):\n",
    "        \"\"\"Read cards from silver layer\"\"\"\n",
    "        self.logger.info(f\"Reading cards from {self.config.cards_path}\")\n",
    "        return self.spark.read.parquet(self.config.cards_path)\n",
    "    \n",
    "    def read_users(self):\n",
    "        \"\"\"Read users from silver layer\"\"\"\n",
    "        self.logger.info(f\"Reading users from {self.config.users_path}\")\n",
    "        return self.spark.read.parquet(self.config.users_path)\n",
    "    \n",
    "    def read_mcc_codes(self):\n",
    "        \"\"\"Read MCC codes from silver layer\"\"\"\n",
    "        self.logger.info(f\"Reading MCC codes from {self.config.mcc_codes_path}\")\n",
    "        return self.spark.read.parquet(self.config.mcc_codes_path)\n",
    "\n",
    "\n",
    "class DailyTransactionAggregator:\n",
    "    \"\"\"Transforms silver data into daily transaction summary\"\"\"\n",
    "    \n",
    "    def __init__(self, spark: SparkSession):\n",
    "        self.spark = spark\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def aggregate_daily_summary(self, transactions_df, users_df, cards_df, mcc_df):\n",
    "        \"\"\"\n",
    "        Aggregate transactions with user, card, and MCC data into daily summary\n",
    "        \n",
    "        Args:\n",
    "            transactions_df: Transactions DataFrame from silver layer\n",
    "            users_df: Users DataFrame from silver layer\n",
    "            cards_df: Cards DataFrame from silver layer\n",
    "            mcc_df: MCC codes DataFrame from silver layer\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with daily aggregated metrics\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting daily aggregation with all tables\")\n",
    "        \n",
    "        # Join all tables\n",
    "        enriched_df = transactions_df \\\n",
    "            .join(users_df, transactions_df.client_id == users_df.user_id, \"left\") \\\n",
    "            .join(cards_df, transactions_df.txn_card_id == cards_df.card_id, \"left\") \\\n",
    "            .join(mcc_df, transactions_df.mcc == mcc_df.mcc_code, \"left\")\n",
    "\n",
    "        enriched_df = enriched_df.cache()\n",
    "        \n",
    "        self.logger.info(f\"Enriched dataset created with {enriched_df.count()} records\")\n",
    "        \n",
    "        # Cast transaction date\n",
    "        enriched_df = enriched_df.withColumn(\n",
    "            \"transaction_date\", \n",
    "            col(\"date\").cast(\"date\")\n",
    "        )\n",
    "        \n",
    "        # Add derived columns\n",
    "        enriched_df = enriched_df.withColumn(\n",
    "            \"is_chip\",\n",
    "            when(col(\"use_chip\").contains(\"Chip\"), 1).otherwise(0)\n",
    "        )\n",
    "        \n",
    "        enriched_df = enriched_df.withColumn(\n",
    "            \"is_swipe\",\n",
    "            when(col(\"use_chip\").contains(\"Swipe\"), 1).otherwise(0)\n",
    "        )\n",
    "        \n",
    "        enriched_df = enriched_df.withColumn(\n",
    "            \"has_error\",\n",
    "            when((col(\"errors\").isNotNull()) & (col(\"errors\") != \"\"), 1).otherwise(0)\n",
    "        )\n",
    "        \n",
    "        enriched_df = enriched_df.withColumn(\n",
    "            \"is_credit_card\",\n",
    "            when(col(\"card_type\").contains(\"Credit\"), 1).otherwise(0)\n",
    "        )\n",
    "        \n",
    "        enriched_df = enriched_df.withColumn(\n",
    "            \"is_debit_card\",\n",
    "            when(col(\"card_type\").contains(\"Debit\"), 1).otherwise(0)\n",
    "        )\n",
    "        \n",
    "        enriched_df = enriched_df.withColumn(\n",
    "            \"is_visa\",\n",
    "            when(col(\"card_brand\") == \"Visa\", 1).otherwise(0)\n",
    "        )\n",
    "        \n",
    "        enriched_df = enriched_df.withColumn(\n",
    "            \"is_mastercard\",\n",
    "            when(col(\"card_brand\") == \"Mastercard\", 1).otherwise(0)\n",
    "        )\n",
    "        \n",
    "        enriched_df = enriched_df.withColumn(\n",
    "            \"is_dark_web\",\n",
    "            when(col(\"card_on_dark_web\") == \"Yes\", 1).otherwise(0)\n",
    "        )\n",
    "        \n",
    "        enriched_df = enriched_df.withColumn(\n",
    "            \"is_male\",\n",
    "            when(col(\"gender\") == \"Male\", 1).otherwise(0)\n",
    "        )\n",
    "        \n",
    "        enriched_df = enriched_df.withColumn(\n",
    "            \"is_female\",\n",
    "            when(col(\"gender\") == \"Female\", 1).otherwise(0)\n",
    "        )\n",
    "        \n",
    "        # Calculate debt to income ratio\n",
    "        enriched_df = enriched_df.withColumn(\n",
    "            \"debt_to_income_ratio\",\n",
    "            when(col(\"yearly_income\") > 0, col(\"total_debt\") / col(\"yearly_income\")).otherwise(0)\n",
    "        )\n",
    "        \n",
    "        # Calculate credit utilization (transaction amount / credit limit)\n",
    "        enriched_df = enriched_df.withColumn(\n",
    "            \"credit_utilization\",\n",
    "            when(col(\"credit_limit\") > 0, \n",
    "                 _abs(col(\"amount\")) / col(\"credit_limit\") * 100).otherwise(0)\n",
    "        )\n",
    "        \n",
    "        # High risk transaction flag (swipe > $1000 or dark web card)\n",
    "        enriched_df = enriched_df.withColumn(\n",
    "            \"is_high_risk\",\n",
    "            when((col(\"is_swipe\") == 1) & (_abs(col(\"amount\")) > 1000), 1)\n",
    "            .when(col(\"is_dark_web\") == 1, 1)\n",
    "            .otherwise(0)\n",
    "        )\n",
    "        \n",
    "        # Identify restaurant and gas station transactions\n",
    "        enriched_df = enriched_df.withColumn(\n",
    "            \"is_restaurant\",\n",
    "            when(col(\"mcc\") == 5812, 1).otherwise(0)\n",
    "        )\n",
    "        \n",
    "        enriched_df = enriched_df.withColumn(\n",
    "            \"is_gas_station\",\n",
    "            when(col(\"mcc\") == 5541, 1).otherwise(0)\n",
    "        )\n",
    "        \n",
    "        # Group by date and aggregate with metrics\n",
    "        daily_summary = enriched_df.groupBy(\"transaction_date\").agg(\n",
    "            # ===== BASIC TRANSACTION METRICS =====\n",
    "            count(\"*\").alias(\"total_transactions\"),\n",
    "            _sum(\"amount\").alias(\"total_revenue\"),\n",
    "            _sum(_abs(col(\"amount\"))).alias(\"total_revenue_absolute\"),\n",
    "            _round(avg(\"amount\"), 2).alias(\"average_transaction_amt\"),\n",
    "            _min(\"amount\").alias(\"min_transaction_amt\"),\n",
    "            _max(\"amount\").alias(\"max_transaction_amt\"),\n",
    "            \n",
    "            # ===== USER DEMOGRAPHICS =====\n",
    "            _round(avg(\"current_age\"), 2).alias(\"avg_customer_age\"),\n",
    "            _round(avg(\"credit_score\"), 2).alias(\"avg_credit_score\"),\n",
    "            _round(avg(\"yearly_income\"), 2).alias(\"avg_yearly_income\"),\n",
    "            _round(avg(\"debt_to_income_ratio\"), 2).alias(\"avg_debt_to_income_ratio\"),\n",
    "            _round((_sum(\"is_male\") / count(\"*\")) * 100, 2).alias(\"pct_male\"),\n",
    "            _round((_sum(\"is_female\") / count(\"*\")) * 100, 2).alias(\"pct_female\"),\n",
    "            \n",
    "            # ===== CARD INSIGHTS =====\n",
    "            _round((_sum(\"is_credit_card\") / count(\"*\")) * 100, 2).alias(\"pct_credit_cards\"),\n",
    "            _round((_sum(\"is_debit_card\") / count(\"*\")) * 100, 2).alias(\"pct_debit_cards\"),\n",
    "            _round((_sum(\"is_visa\") / count(\"*\")) * 100, 2).alias(\"pct_visa\"),\n",
    "            _round((_sum(\"is_mastercard\") / count(\"*\")) * 100, 2).alias(\"pct_mastercard\"),\n",
    "            _sum(\"is_dark_web\").alias(\"cards_on_dark_web_count\"),\n",
    "            _round(avg(\"credit_limit\"), 2).alias(\"avg_credit_limit\"),\n",
    "            _round(avg(\"credit_utilization\"), 2).alias(\"avg_credit_utilization\"),\n",
    "            \n",
    "            # ===== MERCHANT CATEGORY =====\n",
    "            _sum(when(col(\"is_restaurant\") == 1, col(\"amount\")).otherwise(0)).alias(\"restaurant_revenue\"),\n",
    "            _sum(when(col(\"is_gas_station\") == 1, col(\"amount\")).otherwise(0)).alias(\"gas_station_revenue\"),\n",
    "            \n",
    "            # ===== GEOGRAPHIC =====\n",
    "            countDistinct(\"merchant_state\").alias(\"unique_states\"),\n",
    "            countDistinct(\"merchant_city\").alias(\"unique_cities\"),\n",
    "            \n",
    "            # ===== RISK INDICATORS =====\n",
    "            _sum(\"is_high_risk\").alias(\"high_risk_transactions\"),\n",
    "            _sum(when((col(\"is_swipe\") == 1) & (_abs(col(\"amount\")) > 1000), 1).otherwise(0)).alias(\"swipe_over_1000_count\"),\n",
    "            _sum(\"has_error\").alias(\"failed_transactions\"),\n",
    "            \n",
    "            # ===== EXISTING METRICS =====\n",
    "            countDistinct(\"client_id\").alias(\"active_users\"),\n",
    "            countDistinct(\"txn_card_id\").alias(\"active_cards\"),\n",
    "            countDistinct(\"merchant_id\").alias(\"unique_merchants\"),\n",
    "            _sum(\"is_chip\").alias(\"chip_transactions\"),\n",
    "            _sum(\"is_swipe\").alias(\"swipe_transactions\"),\n",
    "            _sum(when(col(\"is_chip\") == 1, col(\"amount\")).otherwise(0)).alias(\"chip_revenue\"),\n",
    "            _sum(when(col(\"is_swipe\") == 1, col(\"amount\")).otherwise(0)).alias(\"swipe_revenue\"),\n",
    "            _sum(\"has_error\").alias(\"total_errors\")\n",
    "        )\n",
    "        \n",
    "        # Calculate top state and city (most transactions)\n",
    "        # Create temp views for window functions\n",
    "        state_window = Window.partitionBy(\"transaction_date\").orderBy(desc(\"state_count\"))\n",
    "        city_window = Window.partitionBy(\"transaction_date\").orderBy(desc(\"city_count\"))\n",
    "        \n",
    "        # Get top state per day\n",
    "        top_states = enriched_df.groupBy(\"transaction_date\", \"merchant_state\") \\\n",
    "            .agg(count(\"*\").alias(\"state_count\")) \\\n",
    "            .withColumn(\"state_rank\", row_number().over(state_window)) \\\n",
    "            .filter(col(\"state_rank\") == 1) \\\n",
    "            .select(\n",
    "                col(\"transaction_date\"),\n",
    "                col(\"merchant_state\").alias(\"top_state\")\n",
    "            )\n",
    "        \n",
    "        # Get top city per day\n",
    "        top_cities = enriched_df.groupBy(\"transaction_date\", \"merchant_city\") \\\n",
    "            .agg(count(\"*\").alias(\"city_count\")) \\\n",
    "            .withColumn(\"city_rank\", row_number().over(city_window)) \\\n",
    "            .filter(col(\"city_rank\") == 1) \\\n",
    "            .select(\n",
    "                col(\"transaction_date\"),\n",
    "                col(\"merchant_city\").alias(\"top_city\")\n",
    "            )\n",
    "        \n",
    "        # Get top MCC category per day\n",
    "        mcc_window = Window.partitionBy(\"transaction_date\").orderBy(desc(\"mcc_revenue\"))\n",
    "        \n",
    "        top_mcc = enriched_df.groupBy(\"transaction_date\", \"description\") \\\n",
    "            .agg(_sum(\"amount\").alias(\"mcc_revenue\")) \\\n",
    "            .withColumn(\"mcc_rank\", row_number().over(mcc_window)) \\\n",
    "            .filter(col(\"mcc_rank\") == 1) \\\n",
    "            .select(\n",
    "                col(\"transaction_date\"),\n",
    "                col(\"description\").alias(\"top_mcc_category\"),\n",
    "                col(\"mcc_revenue\").alias(\"top_mcc_revenue\")\n",
    "            )\n",
    "        \n",
    "        # Join top values back to daily summary\n",
    "        daily_summary = daily_summary \\\n",
    "            .join(top_states, \"transaction_date\", \"left\") \\\n",
    "            .join(top_cities, \"transaction_date\", \"left\") \\\n",
    "            .join(top_mcc, \"transaction_date\", \"left\")\n",
    "        \n",
    "        # Calculate error rate and success rate\n",
    "        daily_summary = daily_summary.withColumn(\n",
    "            \"error_rate\",\n",
    "            _round((col(\"total_errors\") / col(\"total_transactions\")) * 100, 2)\n",
    "        )\n",
    "        \n",
    "        daily_summary = daily_summary.withColumn(\n",
    "            \"success_rate\",\n",
    "            _round(100 - col(\"error_rate\"), 2)\n",
    "        )\n",
    "        \n",
    "        # Add date components\n",
    "        daily_summary = daily_summary.withColumn(\"year\", year(col(\"transaction_date\")))\n",
    "        # daily_summary = daily_summary.withColumn(\"month\", month(col(\"transaction_date\")))\n",
    "        daily_summary = daily_summary.withColumn(\"month\", date_format(col(\"transaction_date\"), \"MMM\"))\n",
    "        daily_summary = daily_summary.withColumn(\"week_of_year\", weekofyear(col(\"transaction_date\")))\n",
    "        daily_summary = daily_summary.withColumn(\n",
    "            \"day_of_week\", \n",
    "            date_format(col(\"transaction_date\"), \"EEEE\")\n",
    "        )\n",
    "        \n",
    "        # Calculate is_weekend\n",
    "        daily_summary = daily_summary.withColumn(\n",
    "            \"is_weekend\",\n",
    "            when(dayofweek(col(\"transaction_date\")).isin([1, 7]), True).otherwise(False)\n",
    "        )\n",
    "        \n",
    "        # Add processing timestamp\n",
    "        daily_summary = daily_summary.withColumn(\n",
    "            \"created_at\",\n",
    "            current_timestamp()\n",
    "        )\n",
    "        \n",
    "        # Select columns in the correct order\n",
    "        final_columns = [\n",
    "            # Date dimensions\n",
    "            \"transaction_date\",\n",
    "            \"year\",\n",
    "            \"month\",\n",
    "            \"week_of_year\",\n",
    "            \"day_of_week\",\n",
    "            \"is_weekend\",\n",
    "            \n",
    "            # Transaction metrics\n",
    "            \"total_transactions\",\n",
    "            \"total_revenue\",\n",
    "            \"total_revenue_absolute\",\n",
    "            \"average_transaction_amt\",\n",
    "            \"min_transaction_amt\",\n",
    "            \"max_transaction_amt\",\n",
    "            \n",
    "            # User demographics\n",
    "            \"avg_customer_age\",\n",
    "            \"avg_credit_score\",\n",
    "            \"pct_male\",\n",
    "            \"pct_female\",\n",
    "            \"avg_yearly_income\",\n",
    "            \"avg_debt_to_income_ratio\",\n",
    "            \n",
    "            # Card insights\n",
    "            \"pct_credit_cards\",\n",
    "            \"pct_debit_cards\",\n",
    "            \"pct_visa\",\n",
    "            \"pct_mastercard\",\n",
    "            \"cards_on_dark_web_count\",\n",
    "            \"avg_credit_limit\",\n",
    "            \"avg_credit_utilization\",\n",
    "            \n",
    "            # Merchant category\n",
    "            \"top_mcc_category\",\n",
    "            \"top_mcc_revenue\",\n",
    "            \"restaurant_revenue\",\n",
    "            \"gas_station_revenue\",\n",
    "            \n",
    "            # Geographic\n",
    "            \"unique_states\",\n",
    "            \"unique_cities\",\n",
    "            \"top_state\",\n",
    "            \"top_city\",\n",
    "            \n",
    "            # Risk indicators\n",
    "            \"high_risk_transactions\",\n",
    "            \"swipe_over_1000_count\",\n",
    "            \"failed_transactions\",\n",
    "            \n",
    "            # Existing metrics\n",
    "            \"active_users\",\n",
    "            \"active_cards\",\n",
    "            \"unique_merchants\",\n",
    "            \"chip_transactions\",\n",
    "            \"swipe_transactions\",\n",
    "            \"chip_revenue\",\n",
    "            \"swipe_revenue\",\n",
    "            \"total_errors\",\n",
    "            \"error_rate\",\n",
    "            \"success_rate\",\n",
    "            \n",
    "            # Metadata\n",
    "            \"created_at\"\n",
    "        ]\n",
    "        \n",
    "        daily_summary = daily_summary.select(*final_columns)\n",
    "        \n",
    "        # Sort by date\n",
    "        daily_summary = daily_summary.orderBy(\"transaction_date\")\n",
    "\n",
    "        daily_summary = daily_summary.cache()\n",
    "        \n",
    "        self.logger.info(f\"aggregation completed: {daily_summary.count()} daily records\")\n",
    "        \n",
    "        return daily_summary\n",
    "\n",
    "\n",
    "class PostgreSQLWriter:\n",
    "    \"\"\"Writes data to PostgreSQL\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def write_dataframe(self, df, mode=\"overwrite\"):\n",
    "        \"\"\"\n",
    "        Write DataFrame to PostgreSQL\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to write\n",
    "            mode: Write mode (overwrite, append)\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Writing {df.count()} records to PostgreSQL table: {self.config.postgres_table}\")\n",
    "        \n",
    "        try:\n",
    "            df.write \\\n",
    "                .jdbc(\n",
    "                    url=self.config.jdbc_url,\n",
    "                    table=self.config.postgres_table,\n",
    "                    mode=mode,  # 'overwrite' will truncate and reload\n",
    "                    properties=self.config.jdbc_properties\n",
    "                )\n",
    "            \n",
    "            self.logger.info(\"Successfully wrote data to PostgreSQL\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to write to PostgreSQL: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class GoldLayerETL:\n",
    "    \"\"\"Main ETL orchestrator for Gold Layer\"\"\"\n",
    "    \n",
    "    def __init__(self, app_name=\"GoldLayerETL\"):\n",
    "        self.app_name = app_name\n",
    "        self.spark = None\n",
    "        self.config = None\n",
    "        self.logger = self._setup_logger()\n",
    "    \n",
    "    def _setup_logger(self):\n",
    "        \"\"\"Setup logging\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        return logging.getLogger(__name__)\n",
    "    \n",
    "    def _create_spark_session(self):\n",
    "        \"\"\"Create Spark session with necessary configurations\"\"\"\n",
    "        self.logger.info(\"Creating Spark session\")\n",
    "        \n",
    "        self.spark = SparkSession.builder \\\n",
    "                        .appName(\"app_silver_to_gold_dly_txn_smry\") \\\n",
    "                        .master(\"spark://spark-master:7077\") \\\n",
    "                        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "                        .config(\"spark.executor.cores\", \"1\") \\\n",
    "                        .config(\"spark.cores.max\", \"2\") \\\n",
    "                        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "                        .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "                        .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin123\") \\\n",
    "                        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "                        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "                        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "                        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "                        .config(\"spark.jars.packages\", \n",
    "                                \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "                                \"com.amazonaws:aws-java-sdk-bundle:1.12.262,\"\n",
    "                                \"org.postgresql:postgresql:42.5.4\") \\\n",
    "                        .getOrCreate()\n",
    "        \n",
    "        self.logger.info(f\"Spark session created: {self.spark.version}\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Execute the complete ETL pipeline\"\"\"\n",
    "        try:\n",
    "            start_time = datetime.now()\n",
    "            self.logger.info(\"=\"*70)\n",
    "            self.logger.info(\"Starting Gold Layer ETL Job\")\n",
    "            self.logger.info(\"=\"*70)\n",
    "            \n",
    "            # Step 1: Initialize\n",
    "            self.config = Config()\n",
    "            self._create_spark_session()\n",
    "            \n",
    "            # Step 2: Read ALL data from Silver layer\n",
    "            reader = SilverDataReader(self.spark, self.config)\n",
    "            \n",
    "            transactions_df = reader.read_transactions()\n",
    "            transactions_df = transactions_df.withColumnRenamed(\"id\", \"txn_id\").withColumnRenamed(\"card_id\", \"txn_card_id\")\n",
    "            \n",
    "            users_df = reader.read_users()\n",
    "            users_df = users_df.withColumnRenamed(\"id\", \"user_id\")\n",
    "            \n",
    "            cards_df = reader.read_cards()\n",
    "            cards_df = cards_df.withColumnRenamed(\"id\", \"card_id\").withColumnRenamed(\"client_id\", \"card_client_id\")\n",
    "            \n",
    "            mcc_df = reader.read_mcc_codes()\n",
    "            \n",
    "            self.logger.info(f\"Loaded - Transactions: {transactions_df.count()}, \"\n",
    "                           f\"Users: {users_df.count()}, \"\n",
    "                           f\"Cards: {cards_df.count()}, \"\n",
    "                           f\"MCC Codes: {mcc_df.count()}\")\n",
    "            \n",
    "            # Step 3: Transform data with metrics\n",
    "            aggregator = DailyTransactionAggregator(self.spark)\n",
    "            daily_summary_df = aggregator.aggregate_daily_summary(\n",
    "                transactions_df, users_df, cards_df, mcc_df\n",
    "            )            \n",
    "            \n",
    "            # Step 4: Show sample data\n",
    "            self.logger.info(\"Sample aggregated data:\")\n",
    "            daily_summary_df.show(3, truncate=False)\n",
    "            \n",
    "            # Step 5: Write to PostgreSQL (truncate and reload)\n",
    "            writer = PostgreSQLWriter(self.config)\n",
    "            writer.write_dataframe(daily_summary_df, mode=\"overwrite\")\n",
    "            \n",
    "            # Step 6: Summary\n",
    "            end_time = datetime.now()\n",
    "            duration = (end_time - start_time).total_seconds()\n",
    "            \n",
    "            self.logger.info(\"=\"*70)\n",
    "            self.logger.info(\"ETL Job Completed Successfully!\")\n",
    "            self.logger.info(f\"Total Duration: {duration} seconds\")\n",
    "            self.logger.info(f\"Daily Records Created: {daily_summary_df.count()}\")\n",
    "            self.logger.info(\"=\"*70)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"ETL Job Failed: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "        \n",
    "        finally:\n",
    "            if self.spark:\n",
    "                self.spark.stop()\n",
    "                self.logger.info(\"Spark session stopped\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Entry point for the ETL job\"\"\"\n",
    "    etl = GoldLayerETL(app_name=\"CreditCard_Gold_ETL\")\n",
    "    etl.run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754a693f-370b-441d-8b3a-b400e0cf9feb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
